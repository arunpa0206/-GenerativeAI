{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go0O8gBZ2yjp",
        "outputId": "72a21457-595f-49ff-c6cb-f3fc19017f8e"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zx9z5gvU20gk"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFviPWsL58qA",
        "outputId": "94d5fac6-648e-4642-f2f0-248b6d9d240a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tDxDGoLz20e0"
      },
      "outputs": [],
      "source": [
        "# Replace 'YOUR_API_KEY' with your actual OpenAI API key\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = '' #Sets your ‘OPENAI_API_KEY’ Environment Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-gTmtyV3pG8",
        "outputId": "999e45c4-0a4b-48ce-d211-aa71111066bf"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-426cBU3pE3",
        "outputId": "6740adda-967d-476b-bb5a-6ac2b377f9c5"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z7r176M3pCO",
        "outputId": "4f33879c-c2c3-4b2e-dcc6-7e377c7fa266"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSm-x0Rr6Sr6",
        "outputId": "b3751c5d-9356-41fc-b2af-011eb6942d02"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R7Vpz2j3o_f",
        "outputId": "8674adab-ccea-4100-eac5-ce621fc32475"
      },
      "outputs": [],
      "source": [
        "# use pypdfloader to load the pdf.\n",
        "loader = PyPDFLoader(\"copy your file path here\")\n",
        "documents_ = loader.load_and_split() # This line uses the load_and_split method of the PyPDFLoader to load the PDF document and split its content into smaller text chunks or documents.\n",
        "\n",
        "# using CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=100, chunk_overlap=20)\n",
        "documents = text_splitter.split_documents(documents_)\n",
        "\n",
        "# Here, a CharacterTextSplitter object is created. This splitter splits the text based on certain parameters:\n",
        "\n",
        "# separator=\"\\n\": It uses newline characters to separate the text chunks.\n",
        "# chunk_size=100: Each text chunk is limited to 100 characters in length.\n",
        "# chunk_overlap=20: There is a 20-character overlap between adjacent chunks.\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "  documents,\n",
        "  embedding=OpenAIEmbeddings(), # It creates a Chroma vector database from the documents variable. It uses an embedding provided by OpenAI to represent the text data as vectors.\n",
        "  persist_directory='./docbot' # This specifies the directory where the vector database should be persisted or saved.\n",
        ")\n",
        "\n",
        "# creates a vector database using Chroma.\n",
        "vectordb.persist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcas9lG89aKV",
        "outputId": "1e515bbb-7cd5-423f-b986-7f167828a39c"
      },
      "outputs": [],
      "source": [
        "vectordb.get(ids=['doc0'], include=['embeddings'])\n",
        "\n",
        "# it fetches the embeddings (vector representations) for the document with the ID 'doc0' from your vector database (vectordb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "V0dgArUy4yRh"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "L-m-3M-P20Q4"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI() #This line of code initializes a ChatGPT model. Here llm stands for \"Large Language Model.\"\n",
        "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)\n",
        "\n",
        "# This memory is designed to store and manage conversation history within a chatbot system. Here's what the parameters mean:\n",
        "\n",
        "# llm=llm: This associates the ChatGPT model (llm) with the conversation memory.\n",
        "# memory_key=\"chat_history\": This specifies a key (or label) for the memory. It's used to identify the conversation history.\n",
        "# return_messages=True: This parameter indicates that the memory should store and return individual messages within the conversation.\n",
        "\n",
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "# This code creates a document retriever from the previously defined vector database (vectordb).\n",
        "# A document retriever is a component that can retrieve documents or information from a database based on certain queries.\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)\n",
        "\n",
        "#This code sets up a conversational retrieval chain (qa)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDdATEJl5SWK",
        "outputId": "64ce1ec7-f769-4c1a-de4a-1cd4a970b75c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'Brief the story of 3-idiots',\n",
              " 'chat_history': [SystemMessage(content='', additional_kwargs={})],\n",
              " 'answer': 'The story of 3 Idiots revolves around three friends named Rancho, Raju, and Farhan who enroll in an elite engineering college. The film explores their journey and the life lessons they learn along the way that cannot be taught through books. The story begins with their entry into the college and their initial experiences with ragging.'}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa(\"Brief the story of 3-idiots\")\n",
        "\n",
        "# write the query which you want to ask from the pdf. For example - Here we are using: 'Brief the story of 3-idiots'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
