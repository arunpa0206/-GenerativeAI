# -*- coding: utf-8 -*-
"""RAG-Chatbot

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JgOW3BA1JxcDBztu4ZAvJ34ykus7eqpM
"""

#!pip install -qU \langchain==0.0.292 \openai==0.28.0 \datasets==2.10.1 \pinecone-client==2.2.4 \tiktoken==0.5.1
# Set OpenAI API key
from dotenv import load_dotenv
import os
load_dotenv()
openai_api_key = os.environ.get('OPENAI_API_KEY')
# Initialize OpenAI Chat model
from langchain.chat_models import ChatOpenAI
chat = ChatOpenAI(openai_api_key=os.environ["OPENAI_API_KEY"], model='gpt-3.5-turbo')
# Document path
document_path = "paris.txt"
# Read document content
with open(document_path, "r", encoding="utf-8") as file:
    document_content = file.read()
# Building the Knowledge Base
import pinecone
from langchain.vectorstores import Pinecone

# Get API key from app.pinecone.io and environment from console
pinecone.init(api_key='c86b72b1-4266-4b63-9b7e-8a7735663780', environment='gcp-starter')
# Initialize the index
import time
index_name = 'llama-2-rag'
if index_name not in pinecone.list_indexes():
    pinecone.create_index(index_name, dimension=300, metric='cosine')
    # Wait for index to finish initialization
    while not pinecone.describe_index(index_name).status['ready']:
        time.sleep(1)
index = pinecone.Index(index_name)
# Create Vector embeddings
from langchain.embeddings.openai import OpenAIEmbeddings
embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
# Embed the document content
embed_document = embed_model.embed_documents([document_content])[0]
# Generate metadata for the document
document_metadata = {'text': document_content, 'source': 'your_source', 'title': 'your_title'}
# Upsert the document into Pinecone
document_id = "custom-document-id"
index.upsert(vectors=[(document_id, embed_document, document_metadata)])
# Vector database
text_field = "text"  # the metadata field that contains our text
# Initialize the vector store object
vectorstore = Pinecone(index, embed_model.embed_query, text_field)
# Query the index and see if we have any relevant information given our question
query = "What is so special about paris?"
vectorstore.similarity_search(query, k=2)
# Connect the output from our vector store to our chatbot
def augment_prompt(query: str):
    # Get top 3 results from the knowledge base
    results = vectorstore.similarity_search(query, k=2)
    # Get the text from the results
    source_knowledge = "\n".join([x.page_content for x in results])
    # Feed into an augmented prompt
    augmented_prompt = f"""Using the contexts below, answer the query.
    Contexts:
    {source_knowledge}
    Query: {query}"""
    return augmented_prompt
print(augment_prompt(query))
# Chat interaction
from langchain.schema import SystemMessage, HumanMessage, AIMessage
messages = [SystemMessage(content="You are a helpful assistant."),
            HumanMessage(content="Hi AI, how are you today?"),
            AIMessage(content="I'm great thank you. How can I help you?"),
            HumanMessage(content="I'd like to understand string theory.")]
# Create a new user prompt with augmented knowledge
prompt = HumanMessage(content=augment_prompt(query))
messages.append(prompt)
# Use the chat model to get a response
res = chat(messages)
print(res.content)
# Example with RAG
prompt_with_rag = HumanMessage(content=augment_prompt("What is the history of paris"))
res_with_rag = chat(messages + [prompt_with_rag])
print(res_with_rag.content)