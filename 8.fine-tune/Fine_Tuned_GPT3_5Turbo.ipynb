{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVoU7VrzGzxn"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install numpy\n",
        "!pip install tiktoken\n",
        "!pip install Gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import tiktoken\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "WysuPFYQHfCq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"\""
      ],
      "metadata": {
        "id": "_ofhSD5BII1W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load CSV data in\n",
        "csv_file_path = '/content/helper-example - Sheet1.csv'\n",
        "cleaned_data = []\n",
        "\n",
        "\n",
        "with open(csv_file_path, 'r', encoding='utf-8-sig') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    for row in csv_reader:\n",
        "        for cell in row:\n",
        "            try:\n",
        "                # Replace square brackets and inner double quotes that are problematic\n",
        "                cell = cell.replace('[\"', '').replace('\"]', '').replace('\\\\\"', '\"')\n",
        "\n",
        "                # Load each cell as a JSON object\n",
        "                cell_json = json.loads(cell)\n",
        "\n",
        "                # Now that the content is clean, append to cleaned_data list\n",
        "                cleaned_data.append(cell_json)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"JSON decode error for cell '{cell}': {e}\")\n",
        "\n",
        "jsonl_file_path = '/content/helper-example-json.jsonl'\n",
        "# Write cleaned data to a JSONL file\n",
        "with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
        "    for item in cleaned_data:\n",
        "        jsonl_file.write(json.dumps(item) + '\\n')"
      ],
      "metadata": {
        "id": "ySonCJu2I1KD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
        "\n",
        "# Next, we specify the data path and open the JSONL file\n",
        "\n",
        "data_path = '/content/helper-example-json.jsonl'\n",
        "\n",
        "# Load dataset\n",
        "with open(data_path) as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# We can inspect the data quickly by checking the number of examples and the first item\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(dataset))\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"]:\n",
        "    print(message)\n",
        "\n",
        "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
        "\n",
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        if not content or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")\n",
        "\n",
        "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
        "\n",
        "# Token counting functions\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
        "\n",
        "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
        "\n",
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
        "\n",
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_EPOCHS = 1\n",
        "MAX_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "\n",
        "# Calculate the estimated cost for fine-tuning\n",
        "cost_per_100k_tokens = 0.80  # Cost for every 100,000 tokens\n",
        "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 100000) * cost_per_100k_tokens\n",
        "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing"
      ],
      "metadata": {
        "id": "-NNdePOiKuKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d46e89-7e09-4b3f-f411-a2ff9eca2125"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples: 85\n",
            "First example:\n",
            "{'role': 'system', 'content': \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"}\n",
            "{'role': 'user', 'content': 'Where is the pool located?'}\n",
            "{'role': 'assistant', 'content': \"Ah, the pool! Great choice. It's on the ground floor, just past the elevators. Make sure you bring your room key for access! Let me know if you need further directions!\"}\n",
            "No errors found\n",
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 3, 3\n",
            "mean / median: 3.0, 3.0\n",
            "p5 / p95: 3.0, 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 67, 102\n",
            "mean / median: 77.01176470588236, 77.0\n",
            "p5 / p95: 70.0, 84.20000000000002\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 15, 48\n",
            "mean / median: 25.235294117647058, 25.0\n",
            "p5 / p95: 19.0, 32.0\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
            "Dataset has ~6546 tokens that will be charged for during training\n",
            "By default, you'll train for 3 epochs on this dataset\n",
            "By default, you'll be charged for ~19638 tokens\n",
            "Estimated cost for fine-tuning: approximately $0.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save the dataset as a JSONL file\n",
        "def save_to_jsonl(conversations, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for conversation in conversations:\n",
        "            json_line = json.dumps(conversation)\n",
        "            file.write(json_line + '\\n')\n",
        "\n",
        "# Specify the path where you want to save the JSONL file in your Google Drive\n",
        "jsonl_file_path = '/content/helper-example-json-clean.jsonl'\n",
        "# Save the dataset to the specified file path\n",
        "save_to_jsonl(dataset, jsonl_file_path)"
      ],
      "metadata": {
        "id": "GME1BGqXNGMQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload data for training\n",
        "training_file_name = '/content/helper-example-json-clean.jsonl'\n",
        "\n",
        "training_response = openai.File.create(\n",
        "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
        ")\n",
        "training_file_id = training_response[\"id\"]\n",
        "\n",
        "#Gives training file id\n",
        "print(\"Training file id:\", training_file_id)"
      ],
      "metadata": {
        "id": "d403QpVVN3BU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf07d104-4158-402c-8419-3f1b0c52a1ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file id: file-2iyMVa3ozHBghGgqKdMaCyI7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Fine-Tuning Job\n",
        "suffix_name = \"trial-bot\"\n",
        "\n",
        "response = openai.FineTuningJob.create(\n",
        "    training_file=training_file_id,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    suffix=suffix_name,\n",
        ")\n",
        "\n",
        "job_id = response[\"id\"]\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "SlZEZYGTOY-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22db52ee-0ec1-456c-d203-8ae213d37bc5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"object\": \"fine_tuning.job\",\n",
            "  \"id\": \"ftjob-lNgRgmWaKyEBJiqotxtc1HYo\",\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"created_at\": 1696673494,\n",
            "  \"finished_at\": null,\n",
            "  \"fine_tuned_model\": null,\n",
            "  \"organization_id\": \"org-7l5AdBnSeEmtXzAqKEKXFjkI\",\n",
            "  \"result_files\": [],\n",
            "  \"status\": \"validating_files\",\n",
            "  \"validation_file\": null,\n",
            "  \"training_file\": \"file-2iyMVa3ozHBghGgqKdMaCyI7\",\n",
            "  \"hyperparameters\": {\n",
            "    \"n_epochs\": \"auto\"\n",
            "  },\n",
            "  \"trained_tokens\": null,\n",
            "  \"error\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list events as fine-tuning progresses\n",
        "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
        "\n",
        "events = response[\"data\"]\n",
        "events.reverse()\n",
        "\n",
        "for event in events:\n",
        "    print(event[\"message\"])"
      ],
      "metadata": {
        "id": "1Xyzue2SPYaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b483007d-cc81-4fb7-a1f1-c3dd8f99b4be"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created fine-tuning job: ftjob-lNgRgmWaKyEBJiqotxtc1HYo\n",
            "Validating training file: file-2iyMVa3ozHBghGgqKdMaCyI7\n",
            "Files validated, moving job to queued state\n",
            "Fine-tuning job started\n",
            "Step 1/255: training loss=1.91\n",
            "Step 11/255: training loss=2.45\n",
            "Step 21/255: training loss=0.87\n",
            "Step 31/255: training loss=1.23\n",
            "Step 41/255: training loss=0.55\n",
            "Step 51/255: training loss=0.93\n",
            "Step 61/255: training loss=0.51\n",
            "Step 71/255: training loss=0.54\n",
            "Step 81/255: training loss=0.96\n",
            "Step 91/255: training loss=0.77\n",
            "Step 101/255: training loss=0.54\n",
            "Step 111/255: training loss=1.28\n",
            "Step 121/255: training loss=0.71\n",
            "Step 131/255: training loss=0.60\n",
            "Step 141/255: training loss=0.40\n",
            "Step 151/255: training loss=0.51\n",
            "Step 161/255: training loss=0.30\n",
            "Step 171/255: training loss=0.26\n",
            "Step 181/255: training loss=0.36\n",
            "Step 191/255: training loss=0.37\n",
            "Step 201/255: training loss=0.47\n",
            "Step 211/255: training loss=0.35\n",
            "Step 221/255: training loss=0.26\n",
            "Step 231/255: training loss=0.39\n",
            "Step 241/255: training loss=0.31\n",
            "Step 251/255: training loss=0.13\n",
            "New fine-tuned model created: ft:gpt-3.5-turbo-0613:personal:trial-bot:86yjgtl7\n",
            "The job has successfully completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#retrieve fine-tune model id\n",
        "response = openai.FineTuningJob.retrieve(job_id)\n",
        "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
        "\n",
        "print(response)\n",
        "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)"
      ],
      "metadata": {
        "id": "QUgVjlQxUMhw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f16837-40cb-41c1-841a-c517388ed525"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"object\": \"fine_tuning.job\",\n",
            "  \"id\": \"ftjob-lNgRgmWaKyEBJiqotxtc1HYo\",\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"created_at\": 1696673494,\n",
            "  \"finished_at\": 1696674127,\n",
            "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:personal:trial-bot:86yjgtl7\",\n",
            "  \"organization_id\": \"org-7l5AdBnSeEmtXzAqKEKXFjkI\",\n",
            "  \"result_files\": [\n",
            "    \"file-7PeOKsUCu402SbzrevnGKGcQ\"\n",
            "  ],\n",
            "  \"status\": \"succeeded\",\n",
            "  \"validation_file\": null,\n",
            "  \"training_file\": \"file-2iyMVa3ozHBghGgqKdMaCyI7\",\n",
            "  \"hyperparameters\": {\n",
            "    \"n_epochs\": 3\n",
            "  },\n",
            "  \"trained_tokens\": 19128,\n",
            "  \"error\": null\n",
            "}\n",
            "\n",
            "Fine-tuned model id: ft:gpt-3.5-turbo-0613:personal:trial-bot:86yjgtl7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test it out!\n",
        "test_messages = []\n",
        "\n",
        "system_message = \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"\n",
        "test_messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "user_message = \"Where should we park\"\n",
        "test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "print(test_messages)"
      ],
      "metadata": {
        "id": "8L7IEZScVi62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c76194d-4741-41c3-90bd-14ddddb72942"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"}, {'role': 'user', 'content': 'Where should we park'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OpenAI Chat Completions\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=fine_tuned_model_id, #can test it against gpt-3.5-turbo to see difference\n",
        "    messages=test_messages,\n",
        "    temperature=0,\n",
        "    max_tokens=500\n",
        ")\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "id": "AJ2FtN_IVyxC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d140c4-82c0-4b38-8497-ec768fc164f4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have a parking lot available for guests. It's located right next to the Airbnb.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test it out!\n",
        "test_messages = []\n",
        "\n",
        "system_message = \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"\n",
        "test_messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "user_message = \"What is the gym timings\"\n",
        "test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "# print(test_messages)\n",
        "\n",
        "#OpenAI Chat Completions\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=fine_tuned_model_id, #can test it against gpt-3.5-turbo to see difference\n",
        "    messages=test_messages,\n",
        "    temperature=0,\n",
        "    max_tokens=500\n",
        ")\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nyicOHtp12U",
        "outputId": "399b3c5f-cfec-4584-f717-965c634dc7ee"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"}, {'role': 'user', 'content': 'What is the gym timings'}]\n",
            "The gym is open from 6 a.m. to 10 p.m. daily. It's located on the second floor. Enjoy your workout!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test it out!\n",
        "test_messages = []\n",
        "\n",
        "system_message = \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"\n",
        "test_messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "user_message = \"What is the pool timings\"\n",
        "test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "# print(test_messages)\n",
        "\n",
        "#OpenAI Chat Completions\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=fine_tuned_model_id, #can test it against gpt-3.5-turbo to see difference\n",
        "    messages=test_messages,\n",
        "    temperature=0,\n",
        "    max_tokens=500\n",
        ")\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXlpUtBMp1qo",
        "outputId": "1cf70e4d-68c4-4e49-b0ac-b7b29e05fec2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"}, {'role': 'user', 'content': 'What is the pool timings'}]\n",
            "The pool is open from 6 a.m. to 10 p.m. daily. Don't forget your swimsuit!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test it out!\n",
        "test_messages = []\n",
        "\n",
        "system_message = \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"\n",
        "test_messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "user_message = \"How far is beach?\"\n",
        "test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "# print(test_messages)\n",
        "\n",
        "#OpenAI Chat Completions\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=fine_tuned_model_id, #can test it against gpt-3.5-turbo to see difference\n",
        "    messages=test_messages,\n",
        "    temperature=0,\n",
        "    max_tokens=500\n",
        ")\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "id": "my5_vRswlfQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3d678e0-71fd-4ef0-d2c4-9c1f60f63e8a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The beach is just a 10-minute walk from the Airbnb. Don't forget your sunscreen!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P1un2K_lle_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradio for a better UI\n",
        "def generate_completion(user_prompt):\n",
        "    hidden_context = \"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": hidden_context},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        max_tokens=100,\n",
        "        temperature=0\n",
        "    )\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "iface = gr.Interface(fn=generate_completion,\n",
        "                     inputs=gr.inputs.Textbox(lines=5, placeholder='Question about the Airbnb?'),\n",
        "                     outputs='text',\n",
        "                     title=\"Chatner The Airbnb Helper\",\n",
        "                     input_labels=\"Question\",\n",
        "                     output_labels=\"Response\")\n",
        "\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "id": "TjYFpQkDWsQQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}